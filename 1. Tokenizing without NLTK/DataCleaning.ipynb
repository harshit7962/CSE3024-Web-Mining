{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading input from a text file and saving it as a string\n",
    "text = \"\"\n",
    "with open('test_file2b.txt') as file:\n",
    "    for line in file:\n",
    "        for word in line.split():  \n",
    "            text= text + \" \" + word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' this is harshit in here again this is a test file to evaulate tokenized results of question 2 part b here i am evaluating results for multiline senteces or paragraphs as we can see the same code works fine for multiline sentences and single line sentences here i have 4 different lines of sentences'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing punctuations from our input file\n",
    "import re\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'harshit', 'in', 'here', 'again', 'this', 'is', 'a', 'test', 'file', 'to', 'evaulate', 'tokenized', 'results', 'of', 'question', '2', 'part', 'b', 'here', 'i', 'am', 'evaluating', 'results', 'for', 'multiline', 'senteces', 'or', 'paragraphs', 'as', 'we', 'can', 'see', 'the', 'same', 'code', 'works', 'fine', 'for', 'multiline', 'sentences', 'and', 'single', 'line', 'sentences', 'here', 'i', 'have', '4', 'different', 'lines', 'of', 'sentences']\n"
     ]
    }
   ],
   "source": [
    "#Printing each token\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2' '4' 'a' 'again' 'am' 'and' 'as' 'b' 'can' 'code' 'different'\n",
      " 'evaluating' 'evaulate' 'file' 'fine' 'for' 'harshit' 'have' 'here' 'i'\n",
      " 'in' 'is' 'line' 'lines' 'multiline' 'of' 'or' 'paragraphs' 'part'\n",
      " 'question' 'results' 'same' 'see' 'senteces' 'sentences' 'single' 'test'\n",
      " 'the' 'this' 'to' 'tokenized' 'we' 'works']\n"
     ]
    }
   ],
   "source": [
    "#Printing unique tokens\n",
    "import numpy as np\n",
    "tokens = np.unique(tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing StopWords\n",
    "stopwords = [\"i\", \"a\", \"am\", \"and\", \"at\", \"for\", \"in\", \"is\", \"my\", \"of\", \"this\"]\n",
    "res = []\n",
    "for x in tokens:\n",
    "    if x not in stopwords:\n",
    "        res.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '4', 'again', 'as', 'b', 'can', 'code', 'different', 'evaluating', 'evaulate', 'file', 'fine', 'harshit', 'have', 'here', 'line', 'lines', 'multiline', 'or', 'paragraphs', 'part', 'question', 'results', 'same', 'see', 'senteces', 'sentences', 'single', 'test', 'the', 'to', 'tokenized', 'we', 'works']\n"
     ]
    }
   ],
   "source": [
    "#Printing cleaned tokens in our input text file\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
